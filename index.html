<!DOCTYPE HTML>

<style>
    #full {
        display: none;
    }
</style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Muheng Li</title>

    <meta name="author" content="Muheng Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
<table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Muheng Li</name>
                </p>
                <p>
                    I am a second year M.S student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. In 2020, I obtained my B.Eng. in the Department of Engineering Physics, Tsinghua University.
                </p>
                <p>
                    I am broadly interested in computer vision and deep learning. My current research focuses on comprehensive video understanding and vision-language learning.
                </p>
                <p style="text-align:center">
                    <a href="mailto:ttlmh@qq.com">Email</a> &nbsp/&nbsp
<!--                    <a href="files/CV_YongmingRao.pdf">CV</a> &nbsp/&nbsp-->
                    <a href="https://scholar.google.com/citations?user=U3J4L6IAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/ttlmh"> Github </a>
                </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:50%;max-width:50%" alt="profile photo" src="images/lmh.jpg">
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                <li style="margin: 5px;">
                    <b>2022-03:</b> 1 paper on vision-language video understanding (<a href="https://github.com/ttlmh/Bridge-Prompt">Bridge-Prompt</a>) is accepted to <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <p><heading>Publications</heading></p>
                <p>
                    * indicates equal contribution
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/bridge_prompt.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos</papertitle>
                <br>
                <strong>Muheng Li</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>, <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>, Zhilan Hu, <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en"> Jianjiang Feng </a>,  <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>

                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                <br>
                <a href="https://github.com/ttlmh/Bridge-Prompt">[Code]</a>
                <br>
                <p> We propose a vision-language prompt-based framework, Bridge-Prompt (Br-Prompt), to model the semantics across multiple adjacent correlated actions, so that it simultaneously exploits both out-of-context and contextual information from a series of ordinal actions in instructional videos.
                </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/OCRL.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Order-Constrained Representation Learning for Instructional Video Prediction</papertitle>
                <br>
                <strong>Muheng Li*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>* , <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>, <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en"> Jianjiang Feng</a>,  <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>

                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2022
                <br>
                <a href="pdfs/OCRL_paper.pdf">[PDF]</a>
                <a>[code] (to come)</a>
                <br>
                <p> We propose a weakly-supervised approach called Order-Constrained Representation Learning (OCRL) together with a special contrastive loss function called StepNCE to predict future actions from instructional videos by observing incomplete steps of actions.
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Honors and Awards</heading>
                <p>
                <li style="margin: 5px;"> 2018 National Scholarship, Tsinghua University</li>
                <li style="margin: 5px;"> 2017 Philip K H Wong Foundation Scholarships, Tsinghua University</li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Academic Services</heading>
                <p>
                <li style="margin: 5px;">
                    <b>Conference Reviewer:</b> ICME 2022
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                </p>
            </td>
        </tr>
        </tbody></table>
    </td>
</tr>
</table>

<center><p>
<div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=yp3s8rdiQW_pbzmBOzWDx2Fv6afIlEpV-k1EZiYIkEY"></script>
</div>
<br>
&copy; Muheng Li | Last updated: Mar 26, 2022
</center></p>
</body>

</html>
