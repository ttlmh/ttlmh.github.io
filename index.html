<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Muheng Li - Clinical AI Architect & Researcher specializing in Physics-Informed Deep Learning for Radiation Oncology.">
    <meta name="keywords" content="Muheng Li, Medical Physics, AI, Deep Learning, Proton Therapy, PSI, ETH Zurich, PyTorch, CUDA">
    <meta name="author" content="Muheng Li">
    <title>Muheng Li | Physics-Informed AI</title>

    <link rel="icon" type="image/jpeg" href="images/icon.png    ">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;800&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
        /* --- 1. CORE VARIABLES & RESET --- */
        :root {
            /* Palette: Dark Slate & Medical Teal & AI Cyan */
            --bg-color: #0f172a;       /* Deep Slate */
            --card-bg: #1e293b;        /* Lighter Slate */
            --text-main: #e2e8f0;      /* Off-white */
            --text-muted: #94a3b8;     /* Muted Grey */
            --accent-cyan: #00e5ff;    /* Electric Cyan (AI/Tech) */
            --accent-teal: #14b8a6;    /* Medical Teal (Trust) */
            --accent-glow: rgba(0, 229, 255, 0.15);
            --border-color: rgba(255, 255, 255, 0.1);
            
            --font-main: 'Inter', sans-serif;
            --font-code: 'Fira Code', monospace;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: var(--font-main);
            background-color: var(--bg-color);
            color: var(--text-main);
            line-height: 1.6;
            -webkit-font-smoothing: antialiased;
        }

        a {
            text-decoration: none;
            color: inherit;
            transition: color 0.2s;
        }

        a:hover {
            color: var(--accent-cyan);
        }

        ul {
            list-style: none;
        }

        /* --- 2. LAYOUT UTILITIES --- */
        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 20px;
        }

        section {
            padding: 80px 0;
            border-bottom: 1px solid var(--border-color);
        }

        /* --- 3. NAVIGATION (Sticky) --- */
        nav {
            position: fixed;
            top: 0;
            width: 100%;
            background: rgba(15, 23, 42, 0.9);
            backdrop-filter: blur(10px);
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
        }

        .nav-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
            height: 70px;
        }

        .logo {
            font-weight: 800;
            font-size: 1.2rem;
            letter-spacing: -0.5px;
            background: linear-gradient(to right, var(--text-main), var(--accent-cyan));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .nav-links {
            display: flex;
            gap: 30px;
        }

        .nav-links a {
            font-size: 0.9rem;
            font-weight: 500;
            color: var(--text-muted);
        }

        .nav-links a:hover {
            color: var(--accent-cyan);
        }

        /* --- 4. HERO SECTION --- */
        #home {
            padding-top: 150px;
            padding-bottom: 100px;
            display: grid;
            grid-template-columns: 1.5fr 1fr; /* Text wider than image */
            gap: 60px;
            align-items: center;
            border-bottom: none;
        }

        .hero-content h1 {
            font-size: 3.5rem;
            line-height: 1.1;
            margin-bottom: 20px;
            font-weight: 800;
        }

        .highlight {
            color: var(--accent-cyan);
        }

        .hero-content p {
            font-size: 1.1rem;
            color: var(--text-muted);
            margin-bottom: 30px;
            max-width: 600px;
        }

        .hero-buttons {
            display: flex;
            gap: 15px;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            padding: 12px 24px;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.95rem;
            transition: all 0.3s ease;
        }

        .btn-primary {
            background: linear-gradient(135deg, var(--accent-teal), var(--accent-cyan));
            color: #0f172a; /* Dark text on bright button */
            border: none;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 15px var(--accent-glow);
        }

        .btn-secondary {
            background: transparent;
            border: 1px solid var(--border-color);
            color: var(--text-main);
        }

        .btn-secondary:hover {
            border-color: var(--accent-cyan);
            color: var(--accent-cyan);
        }

        .hero-image-wrapper {
            position: relative;
        }

        .hero-image-wrapper img {
            width: 100%;
            border-radius: 20px;
            border: 2px solid rgba(255,255,255,0.05);
            box-shadow: 0 20px 40px rgba(0,0,0,0.5);
            filter: grayscale(20%) contrast(110%);
            transition: filter 0.3s;
        }

        .hero-image-wrapper img:hover {
            filter: grayscale(0%);
        }

        /* --- 新增的 Hero 社交图标样式 --- */
        .hero-social {
            display: flex;
            gap: 25px; /* 图标之间的间距 */
            margin-top: 35px; /* 距离上方按钮的距离 */
            align-items: center;
        }

        .hero-social a {
            font-size: 1.6rem; /* 图标大小 */
            color: var(--text-muted);
            transition: all 0.3s ease;
        }

        .hero-social a:hover {
            color: var(--accent-cyan); /* 悬停变为青色 */
            transform: translateY(-3px); /* 悬停轻微上浮 */
        }

        /* --- 5. TECH STACK (Skills Matrix) --- */
        .tech-stack {
            display: flex;
            gap: 40px;
            padding: 30px 0;
            overflow-x: auto;
            border-top: 1px solid var(--border-color);
            border-bottom: 1px solid var(--border-color);
            background: rgba(30, 41, 59, 0.3);
        }
        
        .tech-item {
            display: flex;
            align-items: center;
            gap: 10px;
            color: var(--text-muted);
            font-family: var(--font-code);
            font-size: 0.9rem;
            white-space: nowrap;
        }
        
        .tech-item i {
            color: var(--accent-teal);
        }

        /* --- 6. SECTION HEADERS --- */
        .section-header {
            margin-bottom: 50px;
        }

        .section-header h2 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .section-header h2::after {
            content: "";
            flex: 1;
            height: 1px;
            background: var(--border-color);
        }

        /* --- 6.5 ABOUT SECTION --- */
        .about-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 40px;
            align-items: flex-start;
        }

        .subheading {
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 10px;
        }

        .about-grid p {
            color: var(--text-muted);
            font-size: 0.95rem;
            margin-bottom: 12px;
        }

        .bullet-list {
            list-style: none;
            padding: 0;
            margin: 0 0 20px;
        }

        .bullet-list li {
            font-size: 0.9rem;
            color: var(--text-muted);
            margin-bottom: 6px;
        }

        .bullet-list strong {
            color: var(--text-main);
        }

        /* --- 7. PROJECTS GRID (The "Product" View) --- */
        .projects-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
        }

        .project-card {
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            overflow: hidden;
            transition: transform 0.3s, border-color 0.3s;
            display: flex;
            flex-direction: column;
        }

        .project-card:hover {
            transform: translateY(-5px);
            border-color: var(--accent-cyan);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
        }

        .project-media {
            height: 180px;
            overflow: hidden;
            background: #000;
            position: relative;
        }

        .project-media img {
            width: 100%;
            height: 100%;
            object-fit: contain;
            background-color: #000;
            opacity: 0.9;
            transition: opacity 0.3s, transform 0.5s;
        }
        
        .project-card:hover .project-media img {
            opacity: 1;
            transform: scale(1.05);
        }

        .project-content {
            padding: 25px;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .project-tags {
            display: flex;
            gap: 8px;
            margin-bottom: 15px;
            flex-wrap: wrap;
        }

        .tag {
            font-size: 0.7rem;
            font-family: var(--font-code);
            padding: 4px 8px;
            border-radius: 4px;
            background: rgba(20, 184, 166, 0.1); /* Teal bg */
            color: var(--accent-teal);
            text-transform: uppercase;
        }

        .project-title {
            font-size: 1.25rem;
            font-weight: 700;
            margin-bottom: 10px;
            color: var(--text-main);
        }

        .project-desc {
            font-size: 0.95rem;
            color: var(--text-muted);
            margin-bottom: 20px;
            flex-grow: 1;
        }

        .project-links {
            display: flex;
            gap: 15px;
            padding-top: 15px;
            border-top: 1px solid rgba(255,255,255,0.05);
        }

        .project-links a {
            font-size: 0.9rem;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 6px;
        }

        /* --- 8. PUBLICATIONS LIST --- */
        .section-note {
            font-size: 0.85rem;
            color: var(--text-muted);
            margin-bottom: 20px;
        }

        .pub-list {
            display: flex;
            flex-direction: column;
            gap: 24px;
        }

        .pub-item {
            border-left: 2px solid var(--border-color);
            padding-left: 16px;
        }

        .pub-title {
            font-size: 0.98rem;
            font-weight: 600;
            margin-bottom: 4px;
        }

        .pub-authors {
            font-size: 0.9rem;
            color: var(--text-muted);
            margin-bottom: 4px;
        }

        .pub-meta {
            font-size: 0.85rem;
            color: var(--text-muted);
            font-family: var(--font-code);
            margin-bottom: 6px;
        }

        .pub-summary {
            font-size: 0.9rem;
            color: var(--text-muted);
            margin-top: 4px;
        }

        .pub-links a {
            font-size: 0.85rem;
            margin-right: 12px;
            display: inline-flex;
            align-items: center;
            gap: 4px;
            color: var(--accent-teal);
        }

        .pub-links a:hover {
            color: var(--accent-cyan);
        }

        /* --- 9. NEWS TIMELINE (Simplified) --- */
        .news-item {
            display: grid;
            grid-template-columns: 100px 1fr;
            gap: 20px;
            margin-bottom: 20px;
            align-items: baseline;
        }

        .news-date {
            font-family: var(--font-code);
            font-size: 0.85rem;
            color: var(--accent-teal);
            text-align: right;
        }

        .news-text {
            color: var(--text-muted);
            font-size: 0.95rem;
        }

        .news-text strong {
            color: var(--text-main);
        }

        /* --- 10. FOOTER --- */
        footer {
            padding: 50px 0;
            text-align: center;
            color: var(--text-muted);
            font-size: 0.9rem;
            border-top: 1px solid var(--border-color);
        }

        .social-links {
            display: flex;
            justify-content: center;
            gap: 25px;
            margin-bottom: 20px;
        }

        .social-links a {
            font-size: 1.5rem;
            color: var(--text-muted);
            transition: transform 0.2s;
        }

        .social-links a:hover {
            color: var(--text-main);
            transform: scale(1.1);
        }

        /* --- 11. RESPONSIVE DESIGN --- */
        @media (max-width: 768px) {
            #home {
                grid-template-columns: 1fr;
                text-align: center;
                gap: 40px;
                padding-top: 120px;
            }

            .hero-content h1 {
                font-size: 2.5rem;
            }

            .hero-buttons {
                justify-content: center;
            }

            .hero-social {
                justify-content: center; 
            }

            .hero-image-wrapper {
                max-width: 300px;
                margin: 0 auto;
            }

            .nav-links {
                display: none; /* Can add hamburger menu later */
            }
            
            .tech-stack {
                flex-wrap: wrap;
                justify-content: center;
            }

            .about-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>

    <nav>
        <div class="container nav-content">
            <a href="#home" class="logo">Muheng Li<span style="color:var(--accent-teal)"></span></a>
            <div class="nav-links">
                <a href="#about">About</a>
                <a href="#news">Latest Updates</a>
                <a href="#projects">Selected Projects</a>
                <a href="#publications">Publications</a>
            </div>
        </div>
    </nav>

    <div class="container">
        <section id="home">
            <div class="hero-content">
            <h1>Bridging Physics & <br><span class="highlight">Artificial Intelligence</span></h1>
            <p>
                I am a final-year PhD candidate in Physics (Medical AI) at 
                <span style="color:var(--text-main)">PSI &amp; ETH Zurich</span>, specializing in 
                <strong>generative models</strong> and <strong>physics-informed deep learning</strong> for medical imaging and treatment planning.
            </p>
            <p>
                I develop and evaluate deep generative and representation-learning models for high-dimensional imaging and spatiotemporal data. Using 
                <strong>Diffusion Models</strong>, <strong>Flow Matching</strong>, <strong>Implicit Neural Representations</strong>, and 
                <strong>Transformer</strong> architectures, I build end-to-end ML pipelines for tasks such as high-fidelity image synthesis, real-time motion tracking, and adaptive dose prediction in clinical workflows.
            </p>
                <div class="hero-buttons">
                    <a href="#projects" class="btn btn-primary">
                        View Projects <i class="fa-solid fa-arrow-right" style="margin-left: 8px;"></i>
                    </a>
                    <a href="https://scholar.google.com/citations?user=U3J4L6IAAAAJ&hl=en" target="_blank" class="btn btn-secondary">
                        <i class="fa-solid fa-graduation-cap" style="margin-right: 8px;"></i> Google Scholar
                    </a>
                </div>
                <div class="hero-social">
                    <a href="mailto:muheng.li@psi.ch" title="Email"><i class="fa-solid fa-envelope"></i></a>
                    <a href="https://github.com/ttlmh" target="_blank" title="GitHub"><i class="fa-brands fa-github"></i></a>
                    <a href="https://scholar.google.com/citations?user=U3J4L6IAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="fa-solid fa-graduation-cap"></i></a>
                    <a href="https://www.linkedin.com/in/muheng-li-ba4094287/" target="_blank" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a>
                </div>
            </div>
            <div class="hero-image-wrapper">
                <img src="images/lmh_new.JPG" alt="Muheng Li Profile">
            </div>
        </section>
    </div>

    <div class="tech-stack">
    <div class="container" style="display:flex; justify-content: center; width:100%; flex-wrap: wrap; gap: 20px;">
        
        <div class="tech-item">
            <i class="fa-brands fa-python"></i> PyTorch & Lightning
        </div>

        <div class="tech-item">
            <i class="fa-solid fa-microchip"></i> Python / C++ / CUDA
        </div>

        <div class="tech-item">
            <i class="fa-solid fa-brain"></i> Generative Models (Diffusion/Flows)
        </div>

        <div class="tech-item">
            <i class="fa-solid fa-cube"></i> Geometric DL (NeRF/SDF)
        </div>

        <div class="tech-item">
            <i class="fa-solid fa-atom"></i> Medical Physics (Monte Carlo)
        </div>

        <div class="tech-item">
            <i class="fa-solid fa-server"></i> HPC / Docker / Git
        </div>

    </div>
</div>

    <div class="container">
        <section id="about">
            <div class="section-header">
                <h2>About</h2>
            </div>
            <div class="about-grid">
                <div>
                    <h3 class="subheading">Affiliation &amp; Supervision</h3>
                    <p>
                        I am embedded within the <a href="https://www.psi.ch/en/protontherapy" target="_blank">Center for Proton Therapy (CPT)</a>. 
                        My doctoral research is supervised by 
                        Prof. <a href="https://www.phys.ethz.ch/the-department/people/person-detail.MTI0NzQy.TGlzdC84MzgsMTE3MjU5OTI5OQ==.html" target="_blank">Antony Lomax</a> 
                        and Dr. <a href="https://www.linkedin.com/in/yezhang615/" target="_blank">Ye Zhang</a>, 
                        and is supported by the SNSF project <strong><a href="https://data.snf.ch/grants/grant/212855" target="_blank">EPIC-4DAPT</a></strong> (Grant No. 212855).
                    </p>

                    <h3 class="subheading">Education</h3>
                    <p>
                        Prior to Switzerland, I received my M.S. (2023) and B.S. (2020) degrees from 
                        <strong>Tsinghua University</strong>. During my master's in the Department of Automation, 
                        I focused on <strong>3D generative vision</strong> and <strong>multimodal video understanding</strong> under the guidance of 
                        Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/" target="_blank">Jiwen Lu</a> 
                        and Prof. <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/index.html" target="_blank">Jianjiang Feng</a>.
                    </p>
                </div>
            </div>
        </section>
    </div>

<div class="container">
        <section id="news">
            <div class="section-header">
                <h2>Latest Updates</h2>
            </div>

                <div class="news-item">
                    <div class="news-date">Jul 2025</div>
                    <div class="news-text">
                        <strong>Paper Accepted:</strong> Proof-of-concept study on <strong>Direct MRI-based Proton Dose Calculation</strong> 
                        accepted to <a href="https://www.sciencedirect.com/journal/physics-and-imaging-in-radiation-oncology" target="_blank"><em>Physics and Imaging in Radiation Oncology (phiRO)</em></a>.
                    </div>
                </div>

                <div class="news-item">
                    <div class="news-date">May 2025</div>
                    <div class="news-text">
                        <strong>Paper Accepted:</strong> Work on <strong>Diffusion Schrödinger Bridges Models (DSBM)</strong> for high-quality MR-to-CT synthesis 
                        accepted to <a href="https://aapm.onlinelibrary.wiley.com/journal/24734209" target="_blank"><em>Medical Physics</em></a>.
                    </div>
                </div>

                <div class="news-item">
                    <div class="news-date">Jul 2024</div>
                    <div class="news-text">
                        <strong><i class="fa-solid fa-trophy" style="color:var(--accent-cyan)"></i> Awarded:</strong> Won the 
                        <strong>1st Place Rising Star Award (Best Paper)</strong> at 
                        <a href="https://iccr2024.org/" target="_blank"><em>ICCR 2024</em></a> in Lyon for NGP-DIR work.
                    </div>
                </div>

                <div class="news-item">
                    <div class="news-date">Mar 2023</div>
                    <div class="news-text">
                        <strong>CVPR 2023:</strong> <a href="https://github.com/ttlmh/Diffusion-SDF" target="_blank">Diffusion-SDF</a> accepted to 
                        <a href="https://cvpr2023.thecvf.com/" target="_blank"><em>CVPR 2023</em></a>.
                    </div>
                </div>

                <div class="news-item">
                    <div class="news-date">Mar 2022</div>
                    <div class="news-text">
                        <strong>CVPR 2022:</strong> <a href="https://github.com/ttlmh/Bridge-Prompt" target="_blank">Bridge-Prompt</a> accepted to 
                        <a href="https://cvpr2022.thecvf.com/" target="_blank"><em>CVPR 2022</em></a>.
                    </div>
                </div>

            </div>
        </section>
    </div>
    <div class="container">
        <section id="projects">
            <div class="section-header">
                <h2>Selected Engineering Projects</h2>
            </div>
            
            <div class="projects-grid">
                
                <article class="project-card">
                    <div class="project-media">
                        <img src="images/mind_cropped.gif" alt="MIND Framework">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">Real-Time</span>
                            <span class="tag">Transformer</span>
                            <span class="tag">phiRO 2025</span>
                        </div>
                        <h3 class="project-title">MIND: Neural Dose Engine</h3>
                        <p class="project-desc">
                            A proof-of-concept neural engine that predicts individual proton pencil beam dose distributions directly from MRI. 
                                A Transformer-based model operating on beam’s-eye-view patches achieves Monte Carlo-comparable accuracy 
                                (median 99.8% gamma pass rate at 1&nbsp;mm/1%) while reducing computation time to about 3&nbsp;ms per beam 
                                (a 600x speedup over FRED MC on GPU).                        
                        </p>
                        <div class="project-links">
                            <a href="https://doi.org/10.1016/j.phro.2025.100806" target="_blank"><i class="fa-solid fa-file-lines"></i> Paper</a>
                        </div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-media">
                        <img src="images/dsbm.gif" alt="DSBM Generative AI">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">Generative AI</span>
                            <span class="tag">Diffusion Models</span>
                            <span class="tag">Medical Physics 2025</span>
                        </div>
                        <h3 class="project-title">Diffusion Schrödinger Bridges</h3>
                        <p class="project-desc">
                            A DSBM framework for high-fidelity MR-to-CT synthesis that models an entropic optimal transport 
                            (Schrödinger bridge) between MR and CT distributions. Unlike standard diffusion models starting from pure noise, 
                            DSBM leverages MRI as a structural prior to preserve geometric fidelity and dosimetric accuracy.
                        </p>
                        <div class="project-links">
                            <a href="https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.17898" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                        </div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-media">
                        <img src="images/cpt_dir.gif" alt="NGP Registration">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">Computer Vision</span>
                            <span class="tag">Neural Fields</span>
                            <span class="tag">ICCR Best Paper 2024</span>
                        </div>
                        <h3 class="project-title">Instant Neural Motion Tracking</h3>
                        <p class="project-desc">
                            Adapts Instant Neural Graphics Primitives (NGP) and multi-resolution hash encoding to deformable medical image registration. 
                            Enables on-the-fly motion extraction with sub-second inference for 4D dose accumulation, addressing motion management bottlenecks.
                        </p>
                        <div class="project-links">
                            <a href="https://arxiv.org/pdf/2402.05568" target="_blank"><i class="fa-solid fa-trophy"></i> Awarded Paper</a>
                        </div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-media">
                        <img src="images/4dmr.gif" alt="CPT-4DMR">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">4D MRI</span>
                            <span class="tag">Neural Fields</span>
                            <span class="tag">Preprint</span>
                        </div>
                        <h3 class="project-title">CPT-4DMR: Continuous 4D-MRI</h3>
                        <p class="project-desc">
                            A continuous function representation f(x, y, z, t) for 4D-MRI that eliminates binning artifacts and 
                            enables high-quality volumetric MRI reconstruction at arbitrary respiratory phases, critical for tracking irregular breathing motion.
                        </p>
                        <div class="project-links">
                            <a href="https://arxiv.org/pdf/2509.18427" target="_blank"><i class="fa-solid fa-file-pdf"></i> arXiv</a>
                        </div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-media">
                        <img src="images/diffusion-sdf.gif" alt="Diffusion SDF">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">CVPR 2023</span>
                            <span class="tag">3D Vision</span>
                            <span class="tag">Generative</span>
                        </div>
                        <h3 class="project-title">Diffusion-SDF: Text-to-3D</h3>
                        <p class="project-desc">
                            A generative framework combining an SDF autoencoder with Voxelized Diffusion to synthesize 
                            high-quality 3D shapes from text descriptions, and extendable to text-conditioned shape completion and manipulation.
                        </p>
                        <div class="project-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.pdf" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/ttlmh/Diffusion-SDF" target="_blank"><i class="fa-brands fa-github"></i> Code</a>
                        </div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-media">
                        <img src="images/bridge_prompt.gif" alt="Bridge-Prompt">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">CVPR 2022</span>
                            <span class="tag">Video Understanding</span>
                            <span class="tag">Vision-Language</span>
                        </div>
                        <h3 class="project-title">Bridge-Prompt</h3>
                        <p class="project-desc">
                            A vision-language prompt-based framework for ordinal action understanding in instructional videos. 
                            Models semantics across adjacent correlated actions to exploit both out-of-context and contextual information.
                        </p>
                        <div class="project-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Bridge-Prompt_Towards_Ordinal_Action_Understanding_in_Instructional_Videos_CVPR_2022_paper.pdf" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                            <a href="https://github.com/ttlmh/Bridge-Prompt" target="_blank"><i class="fa-brands fa-github"></i> Code</a>
                        </div>
                    </div>
                </article>
<!-- 
                <article class="project-card">
                    <div class="project-media">
                        <img src="images/uarl.png" alt="UARL">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">IJCAI 2022</span>
                            <span class="tag">Action Segmentation</span>
                        </div>
                        <h3 class="project-title">Uncertainty-Aware Representation Learning</h3>
                        <p class="project-desc">
                            An uncertainty-aware representation learning (UARL) method for action segmentation, 
                            explicitly modeling transitional expressions between action periods via uncertainty learning.
                        </p>
                        <div class="project-links">
                            <a href="https://www.ijcai.org/proceedings/2022/115" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                        </div>
                    </div>
                </article>

                <article class="project-card">
                    <div class="project-media">
                        <img src="images/OCRL.png" alt="OCRL">
                    </div>
                    <div class="project-content">
                        <div class="project-tags">
                            <span class="tag">TCSVT 2022</span>
                            <span class="tag">Video Prediction</span>
                        </div>
                        <h3 class="project-title">Order-Constrained Representation Learning</h3>
                        <p class="project-desc">
                            A weakly-supervised Order-Constrained Representation Learning (OCRL) framework with a contrastive loss (StepNCE) 
                            to predict future actions in instructional videos from incomplete observed steps.
                        </p>
                        <div class="project-links">
                            <a href="pdfs/OCRL_paper.pdf" target="_blank"><i class="fa-solid fa-file-lines"></i> Paper</a>
                        </div>
                    </div>
                </article> -->

            </div>
        </section>
    </div>

    <div class="container">
        <section id="publications">
            <div class="section-header">
                <h2>Selected Publications</h2>
            </div>
            <p class="section-note">* indicates equal contribution.</p>

            <div class="pub-list">
                <article class="pub-item">
                    <div class="pub-title">
                        A proof-of-concept study of direct magnetic resonance imaging-based proton dose calculation for brain tumors via neural networks with Monte Carlo-comparable accuracy
                    </div>
                    <div class="pub-authors">
                        <strong>Muheng Li</strong>, Carla Winterhalter, Xia Li, Sairos Safai, 
                        <a href="https://www.phys.ethz.ch/the-department/people/person-detail.MTI0NzQy.TGlzdC84MzgsMTE3MjU5OTI5OQ==.html" target="_blank">Antony Lomax</a>, 
                        <a href="https://www.linkedin.com/in/yezhang615/?originalSubdomain=ch" target="_blank">Ye Zhang</a>
                    </div>
                    <div class="pub-meta">
                        <em>Physics and Imaging in Radiation Oncology (phiRO)</em>, 2025
                    </div>
                    <div class="pub-links">
                        <a href="https://doi.org/10.1016/j.phro.2025.100806" target="_blank"><i class="fa-solid fa-file-lines"></i> Paper</a>
                    </div>
                    <p class="pub-summary">
                        Presents the MIND neural dose engine that predicts individual proton pencil beam dose distributions directly from MR images, 
                        achieving a median 99.8% gamma pass rate at 1&nbsp;mm/1% while reducing computation to about 3&nbsp;ms per beam.
                    </p>
                </article>

                            <!-- NEW: CBCT-based dose calculation paper -->
            <article class="pub-item">
                <div class="pub-title">
                    Neural network-driven direct CBCT-based dose calculation for head-and-neck proton treatment planning
                </div>
                <div class="pub-authors">
                    <strong>Muheng Li</strong>, Evangelia Choulilitsa, Lisa Fankhauser, 
                    Francesca Albertini, 
                    <a href="https://www.phys.ethz.ch/the-department/people/person-detail.MTI0NzQy.TGlzdC84MzgsMTE3MjU5OTI5OQ==.html" target="_blank">Antony Lomax</a>, 
                    <a href="https://www.linkedin.com/in/yezhang615/?originalSubdomain=ch" target="_blank">Ye Zhang</a>
                </div>
                <div class="pub-meta">
                    <em>Physics in Medicine &amp; Biology (PMB)</em>, 2025
                </div>
                <div class="pub-links">
                    <a href="https://doi.org/10.1088/1361-6560/ae222a" target="_blank"><i class="fa-solid fa-file-lines"></i> Paper</a>
                </div>
                <p class="pub-summary">
                    Develops an xLSTM-based CBCT neural dose engine (CBCT-NN) that predicts proton dose directly on CBCT images, 
                    achieving 95.1&nbsp;&plusmn;&nbsp;2.7% gamma pass rates at 2&nbsp;mm/2% and under-3-minute computation for complete head-and-neck plans, 
                    enabling adaptive workflows without synthetic CT or complex correction pipelines.
                </p>
            </article>

                <article class="pub-item">
                    <div class="pub-title">
                        Diffusion Schrödinger bridge models for high-quality MR-to-CT synthesis for proton treatment planning
                    </div>
                    <div class="pub-authors">
                        <strong>Muheng Li</strong>, Xia Li, Sairos Safai,
                        <a href="https://www.phys.ethz.ch/the-department/people/person-detail.MTI0NzQy.TGlzdC84MzgsMTE3MjU5OTI5OQ==.html" target="_blank">Antony Lomax</a>, 
                        <a href="https://www.linkedin.com/in/yezhang615/?originalSubdomain=ch" target="_blank">Ye Zhang</a>
                    </div>
                    <div class="pub-meta">
                        <em>Medical Physics</em>, 2025
                    </div>
                    <div class="pub-links">
                        <a href="https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.17898" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                    </div>
                    <p class="pub-summary">
                        Introduces DSBM, modeling an entropic optimal transport (Schrödinger bridge) between MR and CT distributions 
                        to achieve superior geometric fidelity and dosimetric accuracy in MR-only proton therapy.
                    </p>
                </article>

                <article class="pub-item">
                    <div class="pub-title">
                        Neural graphics primitives-based deformable image registration for on-the-fly motion extraction
                    </div>
                    <div class="pub-authors">
                        Xia Li, Fabian Zhang, <strong>Muheng Li</strong>, Damien Weber, 
                        <a href="https://www.phys.ethz.ch/the-department/people/person-detail.MTI0NzQy.TGlzdC84MzgsMTE3MjU5OTI5OQ==.html" target="_blank">Antony Lomax</a>, 
                        Joachim Buhmann, 
                        <a href="https://www.linkedin.com/in/yezhang615/?originalSubdomain=ch" target="_blank">Ye Zhang</a>
                    </div>
                    <div class="pub-meta">
                        <em>International Conference on the Use of Computers in Radiation Therapy (ICCR)</em>, 2024
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/pdf/2402.05568" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                    </div>
                    <p class="pub-summary">
                        Adapts Instant Neural Graphics Primitives and multi-resolution hash encoding to enable on-the-fly motion extraction 
                        with sub-second inference for accurate 4D dose accumulation.
                    </p>
                </article>

                <article class="pub-item">
                    <div class="pub-title">
                        CPT-4DMR: Continuous sPatial-Temporal representation for 4D-MRI reconstruction
                    </div>
                    <div class="pub-authors">
                        Xinyang Wu*, <strong>Muheng Li*</strong>, Xia Li*, Orso Pusterla, Sairos Safai, Philippe C. Cattin, 
                        <a href="https://www.phys.ethz.ch/the-department/people/person-detail.MTI0NzQy.TGlzdC84MzgsMTE3MjU5OTI5OQ==.html" target="_blank">Antony Lomax</a>, 
                        <a href="https://www.linkedin.com/in/yezhang615/?originalSubdomain=ch" target="_blank">Ye Zhang</a>
                    </div>
                    <div class="pub-meta">
                        Preprint, 2025
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/pdf/2509.18427" target="_blank"><i class="fa-solid fa-file-pdf"></i> arXiv</a>
                    </div>
                    <p class="pub-summary">
                        Proposes a continuous spatio-temporal representation f(x, y, z, t) for 4D-MRI, enabling reconstruction 
                        at arbitrary respiratory phases and reducing binning artifacts.
                    </p>
                </article>

                <article class="pub-item">
                    <div class="pub-title">
                        Diffusion-SDF: Text-to-shape via voxelized diffusion
                    </div>
                    <div class="pub-authors">
                        <strong>Muheng Li</strong>, <a href="https://duanyueqi.github.io/" target="_blank">Yueqi Duan</a>, 
                        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1" target="_blank">Jie Zhou</a>, 
                        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/" target="_blank">Jiwen Lu</a>
                    </div>
                    <div class="pub-meta">
                        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2212.03293" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/ttlmh/Diffusion-SDF" target="_blank"><i class="fa-brands fa-github"></i> Code</a>
                    </div>
                    <p class="pub-summary">
                        Proposes Diffusion-SDF, combining an SDF autoencoder with voxelized diffusion for text-to-shape synthesis, 
                        and extends it to text-conditioned shape completion and manipulation.
                    </p>
                </article>

                <article class="pub-item">
                    <div class="pub-title">
                        Bridge-Prompt: Towards ordinal action understanding in instructional videos
                    </div>
                    <div class="pub-authors">
                        <strong>Muheng Li</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/" target="_blank">Lei Chen</a>, 
                        <a href="https://duanyueqi.github.io/" target="_blank">Yueqi Duan</a>, Zhilan Hu, 
                        <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en" target="_blank">Jianjiang Feng</a>,  
                        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1" target="_blank">Jie Zhou</a>, 
                        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/" target="_blank">Jiwen Lu</a>
                    </div>
                    <div class="pub-meta">
                        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2203.14104" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/ttlmh/Bridge-Prompt" target="_blank"><i class="fa-brands fa-github"></i> Code</a>
                    </div>
                    <p class="pub-summary">
                        Introduces Bridge-Prompt, a prompt-based framework for ordinal action understanding that models 
                        semantics across multiple adjacent correlated actions in instructional videos.
                    </p>
                </article>

                <article class="pub-item">
                    <div class="pub-title">
                        Uncertainty-aware representation learning for action segmentation
                    </div>
                    <div class="pub-authors">
                        <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/" target="_blank">Lei Chen</a>, 
                        <strong>Muheng Li</strong>, <a href="https://duanyueqi.github.io/" target="_blank">Yueqi Duan</a>, 
                        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1" target="_blank">Jie Zhou</a>, 
                        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/" target="_blank">Jiwen Lu</a>
                    </div>
                    <div class="pub-meta">
                        <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2022
                    </div>
                    <div class="pub-links">
                        <a href="https://www.ijcai.org/proceedings/2022/115" target="_blank"><i class="fa-solid fa-file-pdf"></i> Paper</a>
                    </div>
                    <p class="pub-summary">
                        Proposes an uncertainty-aware representation learning framework to capture transitional expressions 
                        between action periods, improving robustness in action segmentation.
                    </p>
                </article>

                <article class="pub-item">
                    <div class="pub-title">
                        Order-constrained representation learning for instructional video prediction
                    </div>
                    <div class="pub-authors">
                        <strong>Muheng Li*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/" target="_blank">Lei Chen*</a>, 
                        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/" target="_blank">Jiwen Lu</a>, 
                        <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en" target="_blank">Jianjiang Feng</a>,  
                        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1" target="_blank">Jie Zhou</a>
                    </div>
                    <div class="pub-meta">
                        <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2022
                    </div>
                    <div class="pub-links">
                        <a href="pdfs/OCRL_paper.pdf" target="_blank"><i class="fa-solid fa-file-lines"></i> Paper</a>
                    </div>
                    <p class="pub-summary">
                        Presents an Order-Constrained Representation Learning (OCRL) approach with a StepNCE contrastive loss 
                        to predict future actions from partially observed instructional videos in a weakly-supervised manner.
                    </p>
                </article>
            </div>
        </section>
    </div>

    <footer>
        <div class="container">
            <div class="social-links">
                <a href="mailto:muheng.li@psi.ch" title="Email"><i class="fa-solid fa-envelope"></i></a>
                <a href="https://github.com/ttlmh" target="_blank" title="GitHub"><i class="fa-brands fa-github"></i></a>
                <a href="https://scholar.google.com/citations?user=U3J4L6IAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="fa-solid fa-graduation-cap"></i></a>
                <a href="https://www.linkedin.com/in/muheng-li-ba4094287/" target="_blank" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a>
            </div>
            <p>&copy; 2025 Muheng Li. Engineered with <i class="fa-solid fa-code" style="color:var(--accent-teal)"></i> and Physics.</p>
            <p style="margin-top:6px; font-size:0.8rem; color:var(--text-muted);">Last updated: Dec 2025</p>
        </div>
    </footer>

</body>
</html>
