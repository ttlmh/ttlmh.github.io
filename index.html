<!DOCTYPE HTML>

<style>
    #full {
        display: none;
    }
</style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Muheng Li</title>

    <meta name="author" content="Muheng Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
<table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Muheng Li</name>
                </p>
                <p>
                    I am a third year M.S student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>and Prof. <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/index.html"> Jianjiang Feng</a>. I am also in cooperation with Prof. <a href="https://duanyueqi.github.io/"> Yueqi Duan</a>. In 2020, I obtained my B.Eng. in the Department of Engineering Physics, Tsinghua University. Currently, I am applying for Ph.D. positions.
                </p>
                <p>
                    I am broadly interested in computer vision and deep learning. My current research focuses on:
                <li style="margin: 5px;" >
                    <b>Generative computer vision</b>: especially for <b>generative 3D models</b> including 3D shape/scene synthesis, 3D reconstruction from real-world data, and representation learning for 3D shape/scene.
                </li>
                <li style="margin: 5px;" >
                    <b>Comprehensive video learning</b>: learning to acquire, analyze, and predict events/knowledge from multi-modal videos (especially for vision-language).
                </li>
                </p>
                <p style="text-align:center">
                    <a href="mailto:li-mh20@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=U3J4L6IAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/ttlmh"> Github </a>
                </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:50%;max-width:50%" alt="profile photo" src="images/lmh.jpg">
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                <li style="margin: 5px;">
                    <b>2022-03:</b> 1 paper on vision-language video understanding (<a href="https://github.com/ttlmh/Bridge-Prompt">Bridge-Prompt</a>) is accepted to <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <p><heading>Preprint</heading></p>
                <p>
                    * indicates equal contribution
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/diffusion-sdf.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Diffusion-SDF: Text-to-Shape via Voxelized Diffusion</papertitle>
                <br>
                <strong>Muheng Li</strong>, <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
                <br>
                <br>
                <a href="https://arxiv.org/abs/2212.03293">[Paper]</a>
                <a href="https://github.com/ttlmh/Diffusion-SDF">[Code]</a>
                <br>
                <p> We propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. We propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. We also extend our approach to further text-to-shape tasks including text-conditioned shape completion and manipulation.
                </p>
            </td>
        </tr>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <p><heading>Publications</heading></p>
                <p>
                    * indicates equal contribution
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/bridge_prompt.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos</papertitle>
                <br>
                <strong>Muheng Li</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>, <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>, Zhilan Hu, <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en"> Jianjiang Feng </a>,  <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>

                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2203.14104">[Paper]</a>
                <a href="https://github.com/ttlmh/Bridge-Prompt">[Code]</a>
                <br>
                <p> We propose a vision-language prompt-based framework, Bridge-Prompt (Br-Prompt), to model the semantics across multiple adjacent correlated actions, so that it simultaneously exploits both out-of-context and contextual information from a series of ordinal actions in instructional videos.
                </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/uarl.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Uncertainty-Aware Representation Learning for Action Segmentation</papertitle>
                <br>
                <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>, <strong>Muheng Li</strong>,  <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>

                <br>
                <em>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</em>, 2022
                <br>
                <a href="https://www.ijcai.org/proceedings/2022/115">[Paper]</a>
                <a>[Code] (to come)</a>
                <br>
                <p> We propose an uncertainty-aware representation Learning (UARL) method for action segmentation. Specifically, we design the UARL to exploit the transitional expression between two action periods by uncertainty learning.
                </p>
            </td>
        </tr>


        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/OCRL.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Order-Constrained Representation Learning for Instructional Video Prediction</papertitle>
                <br>
                <strong>Muheng Li*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>* , <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>, <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en"> Jianjiang Feng</a>,  <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>

                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2022
                <br>
                <a href="pdfs/OCRL_paper.pdf">[Paper]</a>
                <a>[Code] (to come)</a>
                <br>
                <p> We propose a weakly-supervised approach called Order-Constrained Representation Learning (OCRL) together with a special contrastive loss function called StepNCE to predict future actions from instructional videos by observing incomplete steps of actions.
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Honors and Awards</heading>
                <p>
                <li style="margin: 5px;"> 2022 First Class Scholarship for Graduate Students, Tsinghua University</li>
                <li style="margin: 5px;"> 2019 Science and Innovation Scholarship, Tsinghua University</li>
                <li style="margin: 5px;"> 2018 National Scholarship (top 5%), Tsinghua University</li>
                <li style="margin: 5px;"> 2017 Philip K H Wong Foundation Scholarships (top 15%), Tsinghua University</li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Others</heading>
                <p>
                <li style="margin: 5px;">
                    <b>Conference Reviewer:</b> CVPR, ICME, VCIP
                </li>
                <li style="margin: 5px;">
                    <b>Programming Skills:</b> Python, Matlab, C/C++, Java
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                </p>
            </td>
            </td>
        </tr>
        </tbody></table>
    </td>
</tr>
</table>

<center><p>
<div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=66Xa1lFomeZb_LwkIUH5Ruua7D4Dw_4QwmfH_98wqy4"></script>
</div>
<br>
&copy; Muheng Li | Last updated: Dec 8, 2022
</center></p>
</body>

</html>
