<!DOCTYPE HTML>

<style>
    #full {
        display: none;
    }
</style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Muheng Li</title>

    <meta name="author" content="Muheng Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
<table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
                <p style="text-align:center">
                    <name>Muheng Li</name>
                </p>
                <p>
                    Currently, I am a joint doctoral student at Paul Scherrer Institut (PSI) and ETH Zurich, supervised by Prof. <a href="https://www.phys.ethz.ch/the-department/people/person-detail.MTI0NzQy.TGlzdC84MzgsMTE3MjU5OTI5OQ==.html">Antony Lomax</a> and Dr. <a href="https://www.linkedin.com/in/yezhang615/?originalSubdomain=ch">Ye Zhang</a>. Before, I graduated with a M.S degree from the Department of Automation at Tsinghua University, where I was advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/index.html">Jianjiang Feng</a>. I also had my bachelor's degree at the department of Engineering Physics, Tsinghua University.
                </p>
                <p>
                    Now, I am working at PSIâ€™s Center for Proton Therapy, where I am deeply involved in the intersection of AI technology and radiation therapy. The core of my research is focused on addressing the complex challenges associated with 4D data analysis and modeling.
                </p>
                <p>
                    The dynamic nature of organs in motion during therapy sessions produces intricate 4D data. In my earlier academic journey, my research was concentrated on computer vision and deep learning, specifically generative 3D vision and multi-modal video learning. Building on my background, I am dedicated to advancing the methodologies for 4D data analysis, aiming to enhance the precision and effectiveness of proton therapy for cancer treatment.
                </p>
                <p style="text-align:center">
                    <a href="mailto:muheng.li@psi.ch">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=U3J4L6IAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/ttlmh"> Github </a>
                </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:80%;max-width:80%" alt="profile photo" src="images/lmh_new.JPG">
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p>
                <li style="margin: 5px;">
                    <b>2023-03:</b> 1 paper on generative 3D modeling (<a href="https://github.com/ttlmh/Diffusion-SDF">Diffusion-SDF</a>) is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
                </li>
                <li style="margin: 5px;">
                    <b>2022-03:</b> 1 paper on vision-language video understanding (<a href="https://github.com/ttlmh/Bridge-Prompt">Bridge-Prompt</a>) is accepted to <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>

<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--        <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--                <p><heading>Preprint</heading></p>-->
<!--                <p>-->
<!--                    * indicates equal contribution-->
<!--                </p>-->
<!--            </td>-->
<!--        </tr>-->
<!--        </tbody></table>-->

<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <p><heading>Publications</heading></p>
                <p>
                    * indicates equal contribution
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/diffusion-sdf.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Diffusion-SDF: Text-to-Shape via Voxelized Diffusion</papertitle>
                <br>
                <strong>Muheng Li</strong>, <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2212.03293">[Paper]</a>
                <a href="https://github.com/ttlmh/Diffusion-SDF">[Code]</a>
                <br>
                <p> We propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. We propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. We also extend our approach to further text-to-shape tasks including text-conditioned shape completion and manipulation.
                </p>
            </td>
        </tr>



        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/bridge_prompt.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos</papertitle>
                <br>
                <strong>Muheng Li</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>, <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>, Zhilan Hu, <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en"> Jianjiang Feng </a>,  <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>

                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2203.14104">[Paper]</a>
                <a href="https://github.com/ttlmh/Bridge-Prompt">[Code]</a>
                <br>
                <p> We propose a vision-language prompt-based framework, Bridge-Prompt (Br-Prompt), to model the semantics across multiple adjacent correlated actions, so that it simultaneously exploits both out-of-context and contextual information from a series of ordinal actions in instructional videos.
                </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/uarl.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Uncertainty-Aware Representation Learning for Action Segmentation</papertitle>
                <br>
                <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>, <strong>Muheng Li</strong>,  <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>

                <br>
                <em>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</em>, 2022
                <br>
                <a href="https://www.ijcai.org/proceedings/2022/115">[Paper]</a>
                <a>[Code] (to come)</a>
                <br>
                <p> We propose an uncertainty-aware representation Learning (UARL) method for action segmentation. Specifically, we design the UARL to exploit the transitional expression between two action periods by uncertainty learning.
                </p>
            </td>
        </tr>


        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/OCRL.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Order-Constrained Representation Learning for Instructional Video Prediction</papertitle>
                <br>
                <strong>Muheng Li*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/people/Lei_Chen/"> Lei Chen</a>* , <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu</a>, <a href="https://scholar.google.com/citations?user=qlcjuzcAAAAJ&hl=en"> Jianjiang Feng</a>,  <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou</a>

                <br>
                <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2022
                <br>
                <a href="pdfs/OCRL_paper.pdf">[Paper]</a>
                <a>[Code] (to come)</a>
                <br>
                <p> We propose a weakly-supervised approach called Order-Constrained Representation Learning (OCRL) together with a special contrastive loss function called StepNCE to predict future actions from instructional videos by observing incomplete steps of actions.
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Honors and Awards</heading>
                <p>
                <li style="margin: 5px;"> 2022 First Class Scholarship for Graduate Students, Tsinghua University</li>
                <li style="margin: 5px;"> 2019 Science and Innovation Scholarship, Tsinghua University</li>
                <li style="margin: 5px;"> 2018 National Scholarship (top 5%), Tsinghua University</li>
                <li style="margin: 5px;"> 2017 Philip K H Wong Foundation Scholarships (top 15%), Tsinghua University</li>
                </p>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Others</heading>
                <p>
                <li style="margin: 5px;">
                    <b>Conference Reviewer:</b> CVPR, ICME, VCIP
                </li>
                <li style="margin: 5px;">
                    <b>Programming Skills:</b> Python, Matlab, C/C++, Java
                </li>
                </p>
            </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Website Template</a>
                </p>
            </td>
            </td>
        </tr>
        </tbody></table>
    </td>
</tr>
</table>

<center><p>
<div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=66Xa1lFomeZb_LwkIUH5Ruua7D4Dw_4QwmfH_98wqy4"></script>
</div>
<br>
&copy; Muheng Li | Last updated: Oct 10, 2023
</center></p>
</body>

</html>
